{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sn\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(1)\n",
    "np.random.seed(1)\n",
    "legend_names = {'a': 'Wraith', 'b': 'Bangalore', 'c': 'Caustic', 'g': 'Gibraltar', 'i': 'Lifeline',\n",
    "               'l': 'Bloodhound', 'm': 'Mirage', 'o': 'Octane', 'p': 'Pathfinder', 'r': 'Crypto', 'w': 'Wattson',\n",
    "               'z': '(None)'}\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'rb') as fin:\n",
    "        while True:\n",
    "            try:\n",
    "                data.append((pickle.load(fin), pickle.load(fin)))\n",
    "            except:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "def process_data(data):\n",
    "    data = [(image, label) for image, label in data if label != 'z']\n",
    "    np.random.shuffle(data)\n",
    "    images = np.stack([image for image, _ in data])\n",
    "    images = np.moveaxis(images, -1, 1)\n",
    "    labels = [label for _, label in data]\n",
    "    return images, labels\n",
    "\n",
    "def plot_confusion_matrix(label_names, y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index = label_names, columns = label_names)\n",
    "    plt.figure(figsize = (5, 5))\n",
    "    plt.title('Validation Confusion Matrix')\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = r'C:\\Users\\cidzerda\\Documents\\GitHub\\strevr-data\\unsupervised.pickle'\n",
    "label_file_path = r'C:\\Users\\cidzerda\\Documents\\GitHub\\strevr-data\\unsupervised\\unsupervised.txt'\n",
    "with open(pickle_file_path, 'rb') as fin:\n",
    "    images = np.array([a.reshape(-1) / 255.0 for a in pickle.load(fin)]).astype('float32')\n",
    "with open(label_file_path, 'rt') as fin:\n",
    "    labels = eval(fin.read())\n",
    "print(images.shape, len(labels))\n",
    "print(labels[:9])\n",
    "labels = [next(it.dropwhile(lambda t: i < t[0], reversed(labels)))[1] for i in range(len(images))]\n",
    "print(len(labels), labels[:9], labels[111:119])\n",
    "data = [(a, b) for a, b in zip(images, labels)]\n",
    "images, labels = process_data(data)\n",
    "data.clear()\n",
    "\n",
    "# Convert letter labels into numerical labels.\n",
    "label_names = sorted(Counter(labels))\n",
    "label_dict = {b: a for a, b in enumerate(label_names)}\n",
    "labels = [label_dict[label] for label in labels]\n",
    "print(images.shape, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eighty_percent = round(0.8 * len(images))\n",
    "ninety_percent = round(0.9 * len(images))\n",
    "training_indices = np.arange(0, eighty_percent)\n",
    "validation_indices = np.arange(eighty_percent, ninety_percent)\n",
    "testing_indices = np.arange(ninety_percent, len(images))\n",
    "training_images, training_labels = images[training_indices], [labels[i] for i in training_indices]\n",
    "validation_images, validation_labels = images[validation_indices], [labels[i] for i in validation_indices]\n",
    "testing_images, testing_labels = images[testing_indices], [labels[i] for i in testing_indices]\n",
    "training_images.shape, len(training_labels), validation_images.shape, len(validation_labels), testing_images.shape, len(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "\n",
    "# Define the hyper-parameters of the learning system.\n",
    "batch_size = 1500\n",
    "epochs = 30\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create the network.\n",
    "num_outputs = len(label_names)\n",
    "net = mx.gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(mx.gluon.nn.Flatten())\n",
    "    net.add(mx.gluon.nn.Dense(2048, activation=\"relu\"))\n",
    "    net.add(mx.gluon.nn.Dense(2048, activation=\"relu\"))\n",
    "    net.add(mx.gluon.nn.Dense(num_outputs))\n",
    "\n",
    "# Initialize the network's parameters.\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "print(net)\n",
    "\n",
    "# Define the loss and the trainer.\n",
    "softmax_cross_etropy_loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = mx.gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_labels[:9])\n",
    "print(training_images[0])\n",
    "_ = plt.imshow(np.hstack([training_images[i].reshape(30, 35, 3).take([2,1,0], 2) for i in range(5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run epochs of training and validation.\n",
    "for epoch in range(epochs):\n",
    "    # Run the network on the training data set and determine the training Softmax cross-entopy loss.\n",
    "    cumulative_training_loss = 0\n",
    "    training_predictions = []\n",
    "    for i in range(0, training_images.shape[0], batch_size):\n",
    "        data = mx.ndarray.array(training_images[i:i + batch_size]).as_in_context(ctx)\n",
    "        label = mx.ndarray.array(training_labels[i:i + batch_size]).as_in_context(ctx)\n",
    "        with mx.autograd.record():\n",
    "            output = net(data)\n",
    "            training_predictions = training_predictions + np.argmax(output.asnumpy(), axis=1).tolist()\n",
    "            loss = softmax_cross_etropy_loss(output, label)\n",
    "            cumulative_training_loss = cumulative_training_loss + mx.ndarray.sum(loss)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "    training_loss = cumulative_training_loss / len(training_images)\n",
    "\n",
    "    # Run the network on the validation data set and determine the validation Softmax cross-entopy loss.\n",
    "    cumulative_validation_loss = 0\n",
    "    validation_predictions = []\n",
    "    for i in range(0, validation_images.shape[0], batch_size):\n",
    "        data = mx.ndarray.array(validation_images[i:i + batch_size]).as_in_context(ctx)\n",
    "        label = mx.ndarray.array(validation_labels[i:i + batch_size]).as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        validation_predictions = validation_predictions + np.argmax(output.asnumpy(), axis=1).tolist()\n",
    "        validation_loss = softmax_cross_etropy_loss(output, label)\n",
    "        cumulative_validation_loss = cumulative_validation_loss + mx.ndarray.sum(validation_loss)\n",
    "    validation_loss = cumulative_validation_loss / len(validation_images)\n",
    "\n",
    "    # Calculate training and validation accuracies.\n",
    "    # accuracy = (TP+TN) / (TP+FP+TN+FN)\n",
    "    training_accuracy = accuracy_score(training_labels, training_predictions)\n",
    "    validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "\n",
    "    # Print the summary and plot the confusion matrix after each epoch.\n",
    "    print(\"Epoch {}, training loss: {:.2f}, validation loss: {:.2f}, training accuracy: {:.2f}, validation accuracy: {:.2f}\".format(epoch, training_loss.asnumpy()[0], validation_loss.asnumpy()[0], training_accuracy, validation_accuracy))\n",
    "    plot_confusion_matrix(label_names, validation_labels, validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_predictions = []\n",
    "testing_probabilities = []\n",
    "for i in range(0, testing_images.shape[0], batch_size):\n",
    "    data = mx.ndarray.array(testing_images[i:i + batch_size].astype('float32')).as_in_context(ctx)\n",
    "    output = net(data)\n",
    "    testing_predictions += np.argmax(output.asnumpy(), axis=1).tolist()\n",
    "    testing_probabilities.extend([float(mx.nd.softmax(v)[np.argmax(v.asnumpy())].asnumpy()) for v in output])\n",
    "l = [a == b for a, b in zip(testing_labels, testing_predictions)]\n",
    "print('predicted', sum(l), 'correct of', len(l), '({:.1%})'.format(sum(l) / len(l)))\n",
    "l = [c for a, b, c in zip(testing_labels, testing_predictions, testing_probabilities) if a != b]\n",
    "if l:\n",
    "    print('lowest incorrect probability:  {:.1%}, highest incorrect probability:  {:.1%}'.format(min(l), max(l)))\n",
    "l = [c for a, b, c in zip(testing_labels, testing_predictions, testing_probabilities) if a == b]\n",
    "print('lowest correct probability:  {:.1%}, highest correct probability:  {:.1%}'.format(min(l), max(l)))\n",
    "for i in range(1, 10):\n",
    "    print('{}0th percentile correct probability:  {:.1%}'.format(i, sorted(l)[i * len(l) // 10]))\n",
    "print(testing_images[0].reshape(30, 35, 3).take([2,1,0], 2).shape)\n",
    "for i in range(9):\n",
    "    plt.imshow(testing_images[i].reshape(30, 35, 3).take([2,1,0], 2))\n",
    "    plt.title('{} {:.1%}?\\n{}'.format(legend_names[label_names[testing_predictions[i]]], testing_probabilities[i], legend_names[label_names[testing_labels[i]]]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = r'C:\\Users\\cidzerda\\Documents\\GitHub\\strevr-data\\unsupervised\\unsupervised.model'\n",
    "net.save_parameters(model_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
