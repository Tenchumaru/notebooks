{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import mxnet as mx\n",
    "import mxnet.optimizer as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import utils\n",
    "import mxnet_dataset as dataset\n",
    "import models.mxnet_crnn as crnn\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adadelta=True, adam=False, alphabet='0123456789abcdefghijklmnopqrstuvwxyz', batch_size=64, beta1=0.5, cuda=True, display_interval=500, expr_dir='expr', image_height=32, image_width=100, keep_ratio=False, lr=0.01, manual_seed=1234, n_test_disp=10, nepoch=25, ngpu=1, nh=256, pretrained='', random_sample=False, save_interval=500, train_root='D:\\\\tmp\\\\2858', val_interval=500, val_root='D:\\\\tmp\\\\2859-6', workers=0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train-root', required=True, help='path to dataset')\n",
    "parser.add_argument('--val-root', required=True, help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batch-size', type=int, default=64, help='input batch size')\n",
    "parser.add_argument('--image-height', type=int, default=32, help='the height of the input image to network')\n",
    "parser.add_argument('--image-width', type=int, default=100, help='the width of the input image to network')\n",
    "parser.add_argument('--nh', type=int, default=256, help='size of the lstm hidden state')\n",
    "parser.add_argument('--nepoch', type=int, default=25, help='number of epochs to train for')\n",
    "# TODO(meijieru): epoch -> iter\n",
    "parser.add_argument('--cuda', action='store_true', help='enables cuda')\n",
    "parser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--pretrained', default='', help=\"path to pretrained model (to continue training)\")\n",
    "parser.add_argument('--alphabet', type=str, default='0123456789abcdefghijklmnopqrstuvwxyz')\n",
    "parser.add_argument('--expr-dir', default='expr', help='Where to store samples and models')\n",
    "parser.add_argument('--display-interval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--n-test-disp', type=int, default=10, help='Number of samples to display when test')\n",
    "parser.add_argument('--val-interval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--save-interval', type=int, default=500, help='Interval to be displayed')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='learning rate for Critic, not used by adadealta')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--adam', action='store_true', help='Whether to use adam (default is rmsprop)')\n",
    "parser.add_argument('--adadelta', action='store_true', help='Whether to use adadelta (default is rmsprop)')\n",
    "parser.add_argument('--keep-ratio', action='store_true', help='whether to keep ratio for image resize')\n",
    "parser.add_argument('--manual-seed', type=int, default=1234, help='reproduce experiemnt')\n",
    "parser.add_argument('--random-sample', action='store_true', help='whether to sample the dataset with random sampler')\n",
    "# --adadelta --train-root D:\\tmp\\2858 --val-root D:\\tmp\\2859-6 --cuda --workers 0\n",
    "opt = parser.parse_args(['--adadelta', '--train-root', r'D:\\tmp\\2858', '--val-root', r'D:\\tmp\\2859-6', '--cuda', '--workers', '0'])\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTCLoss = mx.gluon.loss.CTCLoss\n",
    "\n",
    "if not os.path.exists(opt.expr_dir):\n",
    "    os.makedirs(opt.expr_dir)\n",
    "\n",
    "random.seed(opt.manual_seed)\n",
    "np.random.seed(opt.manual_seed)\n",
    "mx.random.seed(opt.manual_seed)\n",
    "\n",
    "if mx.context.num_gpus() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "if opt.cuda:\n",
    "    ctx = mx.gpu()\n",
    "else:\n",
    "    ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.lmdbDataset(root=opt.train_root)\n",
    "assert train_dataset\n",
    "if not opt.random_sample:\n",
    "    sampler = dataset.randomSequentialSampler(train_dataset, opt.batch_size, ctx)\n",
    "else:\n",
    "    sampler = None\n",
    "train_loader = mx.gluon.data.DataLoader(\n",
    "    train_dataset, batch_size=opt.batch_size,\n",
    "    shuffle=sampler is None, sampler=sampler,\n",
    "    num_workers=int(opt.workers),\n",
    "    batchify_fn=dataset.alignCollate(image_height=opt.image_height, image_width=opt.image_width, keep_ratio=opt.keep_ratio))\n",
    "test_dataset = dataset.lmdbDataset(\n",
    "    root=opt.val_root, transform=dataset.resizeNormalize((100, 32)))\n",
    "\n",
    "nclass = len(opt.alphabet) + 1\n",
    "nc = 1\n",
    "\n",
    "converter = utils.strLabelConverter(opt.alphabet)\n",
    "criterion = CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRNN(\n",
      "  (cnn): Sequential(\n",
      "    (conv0): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu0): Activation(relu)\n",
      "    (pooling0): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "    (conv1): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu1): Activation(relu)\n",
      "    (pooling1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "    (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "    (relu2): Activation(relu)\n",
      "    (conv3): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu3): Activation(relu)\n",
      "    (pooling2): MaxPool2D(size=(2, 2), stride=(2, 1), padding=(0, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "    (conv4): Conv2D(None -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batchnorm4): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "    (relu4): Activation(relu)\n",
      "    (conv5): Conv2D(None -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu5): Activation(relu)\n",
      "    (pooling3): MaxPool2D(size=(2, 2), stride=(2, 1), padding=(0, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "    (conv6): Conv2D(None -> 512, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (batchnorm6): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "    (relu6): Activation(relu)\n",
      "  )\n",
      "  (rnn): Sequential(\n",
      "    (0): BidirectionalLSTM(\n",
      "      (rnn): LSTM(None -> 256, TNC, bidirectional)\n",
      "      (embedding): Dense(None -> 256, linear)\n",
      "    )\n",
      "    (1): BidirectionalLSTM(\n",
      "      (rnn): LSTM(None -> 256, TNC, bidirectional)\n",
      "      (embedding): Dense(None -> 37, linear)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = crnn.CRNN(opt.image_height, nc, nclass, opt.nh)\n",
    "net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "if opt.pretrained != '':\n",
    "    print('loading pretrained model from %s' % opt.pretrained)\n",
    "    # TODO\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = nd.zeros((opt.batch_size, 3, opt.image_height, opt.image_height), ctx)\n",
    "text = nd.array((opt.batch_size * 5), ctx)\n",
    "length = nd.array((opt.batch_size,), ctx)\n",
    "\n",
    "# set up optimizer\n",
    "if opt.adam:\n",
    "    optimizer = optim.Adam(learning_rate=opt.lr, beta1=opt.beta1)\n",
    "elif opt.adadelta:\n",
    "    optimizer = optim.AdaDelta()\n",
    "else:\n",
    "    optimizer = optim.RMSProp(learning_rate=opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Image' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2c97d5531860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mloss_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2c97d5531860>\u001b[0m in \u001b[0;36mtrainBatch\u001b[1;34m(net, criterion, optimizer, train_iter)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mcpu_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcpu_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\cidzerda\\documents\\github\\notebooks\\venv\\lib\\site-packages\\mxnet\\gluon\\data\\dataloader.py\u001b[0m in \u001b[0;36msame_process_iter\u001b[1;34m()\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0msame_process_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_sampler\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batchify_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_as_in_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_pinned\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_device_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\notebooks\\mxnet_dataset.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresizeNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\notebooks\\mxnet_dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresizeNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\notebooks\\mxnet_dataset.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\notebooks\\mxnet_dataset.py\u001b[0m in \u001b[0;36mtoTensor\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mtoTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Image' object is not iterable"
     ]
    }
   ],
   "source": [
    "def trainBatch(net, criterion, optimizer, train_iter):\n",
    "    data = next(train_iter)\n",
    "    cpu_images, cpu_texts = data\n",
    "    batch_size = cpu_images.size(0)\n",
    "    utils.loadData(image, cpu_images)\n",
    "    t, l = converter.encode(cpu_texts)\n",
    "    utils.loadData(text, t)\n",
    "    utils.loadData(length, l)\n",
    "\n",
    "    preds = net(image)\n",
    "    preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "    cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "    net.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    return cost\n",
    "\n",
    "\n",
    "for epoch in range(opt.nepoch):\n",
    "    train_iter = iter(train_loader)\n",
    "    i = 0\n",
    "    while i < len(train_loader):\n",
    "        cost = trainBatch(net, criterion, optimizer, train_iter)\n",
    "        loss_avg.add(cost)\n",
    "        i += 1\n",
    "\n",
    "        if i % opt.displayInterval == 0:\n",
    "            print('[%d/%d][%d/%d] Loss: %f' %\n",
    "                  (epoch, opt.nepoch, i, len(train_loader), loss_avg.val()))\n",
    "            loss_avg.reset()\n",
    "\n",
    "        if i % opt.valInterval == 0:\n",
    "            val(net, test_dataset, criterion)\n",
    "\n",
    "        # do checkpointing\n",
    "        if i % opt.saveInterval == 0:\n",
    "            torch.save(\n",
    "                net.state_dict(), '{0}/netCRNN_{1}_{2}.pth'.format(opt.expr_dir, epoch, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
